{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PROJECT IDEA(S)\n",
    "# take ~10000 known fake\n",
    "# take ~10000 known real\n",
    "# combine and take ~25% to put in holdout set - do not use to model - use as verifier of model\n",
    "# feature extraction - \n",
    "# can have 10 different metrics for exclamation marks: \n",
    "# total number of exclamation marks per \n",
    "\n",
    "# look at number of key words: \"outrageous\", \"strong words\"\n",
    "# Q: how strong is the strongest word\n",
    "# unique word count - word frequency\n",
    "# columns: fake / not fake, trustworthiness of source, strength of strongest word found in given article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12999, 19)\n",
      "(422419, 7)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import ttest_ind\n",
    "from matplotlib import rcParams\n",
    "\n",
    "fake_df = pd.DataFrame.from_csv(\"fake.csv\")\n",
    "real_df = pd.DataFrame.from_csv(\"../uci-news-aggregator.csv\")\n",
    "\n",
    "fake_num_rows = fake_df.shape\n",
    "print(fake_num_rows)\n",
    "\n",
    "real_num_rows = real_df.shape\n",
    "print(real_num_rows)\n",
    "\n",
    "# df.head(100)\n",
    "# print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "real_df = real_df.head(12999)\n",
    "# print(real_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bs            11492\n",
      "bias            443\n",
      "conspiracy      430\n",
      "hate            246\n",
      "satire          146\n",
      "state           121\n",
      "junksci         102\n",
      "fake             19\n",
      "Name: type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "counts_by_type = fake_df['type'].value_counts()\n",
    "print(counts_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_by_url = fake_df['main_img_url'].value_counts()\n",
    "# print(counts_by_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(fake_df['spam_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_by_url = fake_df['site_url'].value_counts()\n",
    "# print(counts_by_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_of_spam = fake_df['spam_score'].value_counts()\n",
    "# print(counts_of_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_names = fake_df.columns.tolist()\n",
    "# print(col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_by_replies = fake_df['replies_count'].value_counts()\n",
    "# print(counts_by_replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_by_author = fake_df['author'].value_counts()\n",
    "# print(counts_by_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_by_domain_rank = fake_df['domain_rank'].value_counts()\n",
    "# print(counts_by_domain_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PROJECT IDEA(S)\n",
    "# take ~10000 known fake\n",
    "# take ~10000 known real\n",
    "# combine and take ~25% to put in holdout set - do not use to model - use as verifier of model\n",
    "# feature extraction - \n",
    "# can have 10 different metrics for exclamation marks: \n",
    "# total number of exclamation marks per \n",
    "# \n",
    "# look at number of key words: \"outrageous\", \"strong words\"\n",
    "# Q: how strong is the strongest word\n",
    "# unique word count - word frequency\n",
    "# columns: fake / not fake, trustworthiness of source, strength of strongest word found in given article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12999, 19)\n"
     ]
    }
   ],
   "source": [
    "# create new \"id\" column in df \n",
    "# reorder column names, setting \"id\" as first column and delete \"uuid\" col \n",
    "fake_df['id'] = range(1, len(fake_df) + 1)\n",
    "fake_df = fake_df.set_index('id')\n",
    "fake_df = fake_df[['site_url', 'domain_rank', 'author', 'published', 'title', 'thread_title', 'text', 'ord_in_thread', 'crawled', 'country', 'language', 'spam_score', 'main_img_url', 'replies_count', 'participants_count', 'likes', 'comments', 'shares', 'type']]\n",
    "print(fake_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract text body from fake and save to file\n",
    "fake_text_only = fake_df[['text']].copy()\n",
    "\n",
    "# replace all carriage returns and tabs with spaces\n",
    "for i in range(1, len(fake_text_only) + 1):\n",
    "    text = fake_text_only.loc[i, 'text']\n",
    "    if type(text) != float:\n",
    "        text = text.split(\"\\n\")\n",
    "        text = \" \".join(text)\n",
    "        text = text.split(\"\\t\")\n",
    "        text = \" \".join(text)\n",
    "        fake_text_only.set_value(i, 'text', text)\n",
    "\n",
    "fake_text_only.to_csv(\"fake_body_only.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count ratio of number of exclamation marks to words in the given string\n",
    "def count_ratio_exclams(string):\n",
    "    exclam = '!'\n",
    "    space = \" \"\n",
    "    num_exclams = string.count(exclam)\n",
    "    num_spaces = string.count(space)\n",
    "    if num_spaces == 0:\n",
    "        return num_exclams\n",
    "    else:\n",
    "        return num_exclams / num_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute the ratio of exclams to question marks + periods in the given string\n",
    "def exclam_ratio_text_body(string):\n",
    "    exclam = '!'\n",
    "    period = '.'\n",
    "    question = '?'\n",
    "    num_exclams = string.count(exclam)\n",
    "    num_period = string.count(period)\n",
    "    num_question = string.count(question)\n",
    "    if num_period + num_question == 0:\n",
    "        return num_exclams\n",
    "    return num_exclams / (num_period + num_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create new empty column for ratio_exclam_in_title\n",
    "fake_df.assign(ratio_exclam_in_title=0)\n",
    "  \n",
    "# REMOVE ROWS THAT HAVE NAN thread_title\n",
    "\n",
    "fake_df = fake_df[fake_df['thread_title'].notnull()]\n",
    "print(len(fake_df))\n",
    "\n",
    "# correct id labels\n",
    "fake_df['id'] = range(1, len(fake_df) + 1)\n",
    "fake_df = fake_df.set_index('id')\n",
    "\n",
    "for i in range(1, len(fake_df) + 1):\n",
    "    thread_title = fake_df.loc[i, 'thread_title']\n",
    "    count = count_ratio_exclams(thread_title)\n",
    "    fake_df.set_value(i, 'ratio_exclam_in_title', count)\n",
    "\n",
    "# counts_by_title_exclams = fake_df.total_exclam_in_title.value_counts()\n",
    "# print(counts_by_title_exclams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create new empty column for total_exclam_in_text count\n",
    "fake_df.assign(ratio_exclam_in_text_body=0)\n",
    "  \n",
    "# REMOVE ROWS THAT HAVE NAN thread_title\n",
    "fake_df = fake_df[fake_df['text'].notnull()]\n",
    "print(len(fake_df))\n",
    "\n",
    "# correct id labels\n",
    "fake_df['id'] = range(1, len(fake_df) + 1)\n",
    "fake_df = fake_df.set_index('id')\n",
    "\n",
    "for i in range(1, len(fake_df) + 1):\n",
    "    text = fake_df.loc[i, 'text']\n",
    "    count = exclam_ratio_text_body(text)\n",
    "    fake_df.set_value(i, 'ratio_exclam_in_text_body', count)\n",
    "\n",
    "ratio_in_text_body = fake_df.ratio_exclam_in_text_body.value_counts()\n",
    "# print(ratio_in_text_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create new empty column for ratio_exclams_in_text count\n",
    "fake_df.assign(ratio_exclams_in_text=0)\n",
    "\n",
    "# compute the ratio of exclamation marks to other sentence terminating punctionation\n",
    "# and store in column \"ratio_exclams_in_text\"\n",
    "for i in range(1, len(fake_df) + 1):\n",
    "    text = fake_df.loc[i, 'text']\n",
    "    count = exclam_ratio_text_body(text)\n",
    "    fake_df.set_value(i, 'ratio_exclams_in_text_body', count)\n",
    "\n",
    "counts_ratio_exclams = fake_df.ratio_exclams_in_text.value_counts()\n",
    "# print(counts_ratio_exclams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create new empty column for ratio_exclam_in_title count\n",
    "real_df.assign(ratio_exclam_in_title=0)\n",
    "\n",
    "# compute the ratio of exclamation marks to other sentence terminating punctionation\n",
    "# and store in column \"ratio_exclam_in_title\"\n",
    "for i in range(1, len(real_df) + 1):\n",
    "    thread_title = real_df.loc[i, 'TITLE']\n",
    "    count = count_ratio_exclams(thread_title)\n",
    "    real_df.set_value(i, 'ratio_exclam_in_title', count)\n",
    "\n",
    "counts_ratio_exclams = real_df.ratio_exclam_in_title.value_counts()\n",
    "print(counts_ratio_exclams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create series of total exclamation counts in each row's title\n",
    "# for index, row in df.iterrows():\n",
    "#     count = count_total_exclamation(row.title)\n",
    "#     print(count)\n",
    "#     df.loc[:,'total_crime'] = df.apply(get_total_crime, axis=1)\n",
    "#     df.loc[index, row.total_exclam_in_title] = count\n",
    "\n",
    "\n",
    "# df.loc[:, 'total_exclam_in_title'] = df.apply(count_total_exclams, axis=1)    \n",
    "# count_title_exclams = df['total_exclam_in_title'].value_counts()\n",
    "# print(count_title_exclams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make  copy of the fake_df containing only the thread_title & site_url\n",
    "sub_fake_df = fake_df[['thread_title', 'site_url', 'ratio_exclam_in_title']].copy()\n",
    "\n",
    "# replace all carriage returns and tabs with spaces\n",
    "for i in range(1, len(sub_fake_df) + 1):\n",
    "    title = sub_fake_df.loc[i, 'thread_title']\n",
    "    title = title.split(\"\\n\")\n",
    "    title = \" \".join(title)\n",
    "    title = title.split(\"\\t\")\n",
    "    title = \" \".join(title)\n",
    "    sub_fake_df.set_value(i, 'thread_title', title)\n",
    "\n",
    "# replace all carriage returns and tabs with spaces    \n",
    "for i in range(1, len(sub_fake_df) + 1):\n",
    "    url = sub_fake_df.loc[i, 'site_url']\n",
    "    url = url.split(\"\\n\")\n",
    "    url = \" \".join(url)\n",
    "    url = url.split(\"\\t\")\n",
    "    url = \" \".join(url)\n",
    "    sub_fake_df.set_value(i, 'site_url', url)\n",
    "\n",
    "# turn all tabs into spaces\n",
    "# x = \"The bananas are yellow and green\"\n",
    "# x = x.split(\" \")\n",
    "# print(x)\n",
    "# x = \"+\".join(x)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_real_df = real_df[['TITLE', 'URL', 'ratio_exclam_in_title']].copy()\n",
    "sub_real_df = sub_real_df.head(12941)\n",
    "\n",
    "# correct id labels\n",
    "sub_real_df['id'] = range(1, len(sub_real_df) + 1)\n",
    "sub_real_df = sub_real_df.set_index('id')\n",
    "\n",
    "# replace all carriage returns and tabs with spaces\n",
    "for i in range(1, len(sub_real_df) + 1):\n",
    "    title = sub_real_df.loc[i, 'TITLE']\n",
    "    title = title.split(\"\\n\")\n",
    "    title = \" \".join(title)\n",
    "    title = title.split(\"\\t\")\n",
    "    title = \" \".join(title)\n",
    "    sub_real_df.set_value(i, 'TITLE', title)\n",
    "\n",
    "# replace all carriage returns and tabs with spaces    \n",
    "for i in range(1, len(sub_real_df) + 1):\n",
    "    url = sub_real_df.loc[i, 'URL']\n",
    "    url = url.split(\"\\n\")\n",
    "    url = \" \".join(url)\n",
    "    url = url.split(\"\\t\")\n",
    "    url = \" \".join(url)\n",
    "    sub_real_df.set_value(i, 'URL', url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create new column, 'TARGET' with 1 fake and 0 for real\n",
    "sub_fake_df['TARGET'] = 1\n",
    "sub_fake_df = sub_fake_df.rename(columns = {'thread_title':'TITLE', 'site_url':'URL'})\n",
    "sub_real_df['TARGET'] = 0\n",
    "\n",
    "# combine the two dataframes\n",
    "combined_df = sub_fake_df.append(sub_real_df)\n",
    "\n",
    "# reorder the id index of the combined_df set\n",
    "# correct id labels\n",
    "combined_df['id'] = range(1, len(combined_df) + 1)\n",
    "combined_df = combined_df.set_index('id')\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "combined_df = shuffle(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert combined_df into a new TAB DELIMITED csv\n",
    "combined_df.to_csv(\"cleaned_combined_dataset.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create two sets of data: first is TRAINING SET: 75% OF DATA, 25% VALIDATOR via random num generator\n",
    "sampler = np.random.rand(len(combined_df)) < 0.75\n",
    "training_set = combined_df[sampler]\n",
    "test_set = combined_df[~sampler]\n",
    "\n",
    "#print(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set.to_csv(\"training_set.csv\", sep='\\t', index=False)\n",
    "test_set.to_csv(\"holdout_set.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stem the \"fake news\" data\n",
    "ps = PorterStemmer()\n",
    "fake_blob = {} \n",
    "real_blob = {}\n",
    "for i in range(len(training_set['TARGET'])):\n",
    "    try:\n",
    "        ss8 = str(training_set['TITLE'].iloc[i].encode('utf8'))\n",
    "    except:\n",
    "        ss8 = \"\"\n",
    "    words = word_tokenize(ss8)\n",
    "    x = set()\n",
    "    for w in words:\n",
    "        x.add(ps.stem(w).lower())\n",
    "\n",
    "    # if this is a 'fake' row entry\n",
    "    if training_set['TARGET'].iloc[i] == 1:\n",
    "        for stword in x:\n",
    "            if stword in fake_blob:\n",
    "                fake_blob[stword] = fake_blob[stword] + 1\n",
    "                #print(stword, \" \", fake_blob[stword])\n",
    "            else:\n",
    "                fake_blob.setdefault(stword, 1)\n",
    "                #print(stword,\" \", fake_blob[stword])\n",
    "                \n",
    "    # we found a 'real' row entry\n",
    "    else:\n",
    "        for stword in x:\n",
    "            if stword in real_blob:\n",
    "                real_blob[stword] = real_blob[stword] + 1\n",
    "            else:\n",
    "                real_blob.setdefault(stword, 1)\n",
    "# print(fakeBlob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # stem the \"real news\" data    \n",
    "# goodBlob = {}\n",
    "# for i in range(len(sub_real_df['TITLE'])):\n",
    "#     try:\n",
    "#         ss8 = str(sub_real_df['TITLE'].iloc[i].encode('utf8'))\n",
    "#     except:\n",
    "#         pass\n",
    "#     words = word_tokenize(ss8)\n",
    "#     x = set()\n",
    "#     for w in words:\n",
    "#         x.add(ps.stem(w).lower())\n",
    "\n",
    "#     for stword in x:\n",
    "#         if stword in goodBlob:\n",
    "#             goodBlob[stword] = goodBlob[stword] + 1\n",
    "#             #print(stword, \" \", goodBlob[stword])\n",
    "#         else:\n",
    "#             goodBlob.setdefault(stword,1)\n",
    "#             #print(stword,\" \", goodBlob[stword])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "riskdict = {}\n",
    "for word in fake_blob:\n",
    "    if word in real_blob:\n",
    "        count = (fake_blob[word] + real_blob[word])\n",
    "    else:\n",
    "        count = fake_blob[word]\n",
    "    if count >= 10:\n",
    "        riskdict[word] = fake_blob[word] / count\n",
    "\n",
    "for word in real_blob:\n",
    "    if word not in fake_blob and real_blob[word] >= 10:\n",
    "        riskdict[word] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add four columns to the results_df: fakeaggregate, goodaggregate, riskword, safeword\n",
    "results_df = combined_df.copy()\n",
    "results_df['fake_aggregate'] = 0\n",
    "results_df['good_aggregate'] = 0\n",
    "results_df['risk_word'] = 0\n",
    "results_df['safe_word'] = 0\n",
    "\n",
    "print(results_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "riskword = \"\"\n",
    "for i in range(len(results_df['TITLE'])):\n",
    "    fakeaggregate = 0\n",
    "    goodaggregate = 0\n",
    "    riskyword = 0\n",
    "    safeword = 1\n",
    "    try:\n",
    "        ss8 = str(results_df['TITLE'].iloc[i].encode('utf8'))\n",
    "    except:\n",
    "        ss8 = \"\"\n",
    "    words = word_tokenize(ss8)\n",
    "    x = set()\n",
    "    for w in words:\n",
    "        x.add(ps.stem(w).lower())\n",
    "\n",
    "    for stword in x:\n",
    "        if stword in riskdict:\n",
    "            if riskdict[stword] > riskyword:\n",
    "                riskword = riskdict[stword]\n",
    "            if riskdict[stword] < safeword:\n",
    "                safeword = riskdict[stword]\n",
    "\n",
    "        if stword in fake_blob:\n",
    "            fakeaggregate = fake_blob[stword] + fakeaggregate\n",
    "\n",
    "        if stword in real_blob:\n",
    "            goodaggregate = real_blob[stword] + goodaggregate\n",
    "    # update the results to results_df, training_df, test_df        \n",
    "    results_df.set_value(i, 'fake_aggregate', fakeaggregate)\n",
    "    results_df.set_value(i, 'good_aggregate', goodaggregate)\n",
    "    results_df.set_value(i, 'risk_word', riskword)\n",
    "    results_df.set_value(i, 'safe_word', safeword)\n",
    "    \n",
    "print(results_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create two sets of data: first is TRAINING SET: 75% OF DATA, 25% VALIDATOR via random num generator\n",
    "sampler = np.random.rand(len(results_df)) < 0.75\n",
    "new_training = results_df[sampler]\n",
    "new_test = results_df[~sampler]\n",
    "\n",
    "# new_training = new_training[['TITLE', 'TARGET', 'fake_aggregate', 'good_aggregate', 'risk_word', 'safe_word']]\n",
    "new_training = new_training[['TARGET', 'ratio_exclam_in_title', 'fake_aggregate', 'good_aggregate', 'risk_word', 'safe_word']]\n",
    "print(new_training)\n",
    "\n",
    "holdout_title_features = new_test[['TITLE', 'TARGET', 'ratio_exclam_in_title', 'fake_aggregate', 'good_aggregate', 'risk_word', 'safe_word']]\n",
    "\n",
    "new_training.to_csv(\"new_training.csv\", sep='\\t', index=False)\n",
    "new_test.to_csv(\"new_holdout.csv\", sep='\\t', index=False)\n",
    "\n",
    "holdout_title_features.to_csv(\"holdout_title_features.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
